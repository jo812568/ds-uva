{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Searching Unstructured and Structured Data #\n",
    "## Assignment 1: Retrieval models [100 points] ##\n",
    "**TA**: Nikos Voskarides (n.voskarides@uva.nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this assignment you will get familiar with basic information retrieval concepts. You will implement and evaluate different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Monday, 22/1, at 23:59**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](https://piazza.com/class/ixoz63p156g1ts).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import pyndri\n",
    "import collections\n",
    "import io\n",
    "import sys\n",
    "from scipy import spatial\n",
    "import copy\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n",
      "We got 0 empty rankings in total\n"
     ]
    }
   ],
   "source": [
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    empty_rankings = 0\n",
    "    \n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            empty_rankings = empty_rankings+1\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        #print(\"object_assesments[0][1] is \")\n",
    "        #print(object_assesments[0][1])\n",
    "        if (not(isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes))):\n",
    "            print(object_assesments[0][1])\n",
    "        \n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "            \n",
    "    print(\"We got \"+str(empty_rankings)+\" empty rankings in total\")\n",
    "        #print(\"Just_finished write_run\")\n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52', 'students', 'arrested', 'takeover', 'university', 'massachusetts', 'building', 'fifty', 'two', 'students', 'arrested', 'tuesday', 'evening', 'occupying', 'university', 'massachusetts', 'building', 'overnight', 'protest', 'defense', 'department', 'funded', 'research', 'new', 'york', 'city', 'thousands', 'city', 'college', 'students', 'got', 'unscheduled', 'holiday', 'demonstrators', 'occupied', 'campus', 'administration', 'building', 'protest', 'possible', 'tuition', 'increases', 'prompting', 'officials', 'suspend', 'classes', '60', 'police', 'riot', 'gear', 'arrived', 'university', 'massachusetts', '5', 'p', 'm', 'two', 'hours', 'later', 'bus', 'drove', 'away', '29', 'students', 'camped', 'memorial', 'hall', 'students', 'charged', 'trespassing', '23', 'students', 'arrested', 'lying', 'bus', 'prevent', 'leaving', 'police', '300', 'students', 'stood', 'building', 'chanting', 'looking', 'students', 'hall', 'arrested', '35', 'students', 'occupied', 'memorial', 'hall', '1', 'p', 'm', 'monday', 'declined', 'offer', 'meet', 'administrators', 'provosts', 'office', 'tuesday', 'morning', 'presented', 'list', 'demands', 'halt', 'defense', 'department', 'research', '25', '000', 'student', 'campus', '40', 'students', 'left', 'building', 'tuesday', 'morning', 'university', 'administrators', 'told', 'arrested', '5', 'p', 'm', 'university', 'spokeswoman', 'jeanne', 'hopkins', 'takeover', 'second', 'western', 'massachusetts', 'campus', 'seven', 'protesters', 'arrested', 'april', '19', 'charges', 'disorderly', 'conduct', 'trespassing', 'demonstrating', 'military', 'funded', 'research', 'campus', 'particularly', 'research', 'anthrax', 'research', 'university', 'non', 'classified', 'researchers', 'make', 'work', 'public', 'university', 'rules', '11', '6', 'million', '22', 'percent', 'grant', 'money', 'received', 'university', 'came', 'defense', 'department', '1988', 'school', 'chancellor', 'joseph', 'd', 'duffey', 'issued', 'statement', 'telling', 'students', 'research', 'continue', 'campus', 'school', 'administrators', 'decide', 'differently', 'policy', 'negotiated', 'students', 'duffey', 'latest', 'occupation', 'began', 'students', 'rallying', 'monday', 'student', 'union', 'military', 'research', 'marched', 'administration', 'building', 'ducked', 'memorial', 'hall', 'en', 'route', 'followed', 'members', 'local', 'chapter', 'american', 'friends', 'service', 'committee', 'contended', 'research', 'dangerous', 'town', 'promotes', 'militarism', 'banned', 'university', 'argued', 'purpose', 'anthrax', 'research', 'peaceful', 'strain', 'bacteria', 'non', 'virulent', 'study', 'school', '23', 'years', 'incident', 'amherst', 'health', 'board', 'scheduled', 'hearing', 'wednesday', 'question', 'safety', 'anthrax', 'research', 'tuesday', 'time', '1969', 'classes', 'city', 'college', 'new', 'york', 'canceled', 'student', 'protests', 'school', 'spokesman', 'charles', 'deciccio', 'protesters', 'demanding', 'face', 'face', 'meeting', 'gov', 'mario', 'cuomo', 'feared', 'tuition', 'college', '1', '250', 'increased', 'college', 'staff', 'reduced', 'state', 'budget', 'cuts', 'governor', 'immediate', 'comment', 'tuition', 'set', 'deciccio']\n",
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "index = pyndri.Index('index/')\n",
    "doc_no = index.maximum_document() - index.document_base()\n",
    "token2id, id2token, _ = index.get_dictionary()\n",
    "print([id2token[word_id] for word_id in example_document[1] if word_id > 0])\n",
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task 1: Implement and compare lexical IR methods [40 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html). **[5 points]**\n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of 𝛌 in the range [0.1, 0.5, 0.9]). **[10 points]**\n",
    "    - Dirichlet Prior (explore different values of 𝛍 [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of 𝛅 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[10 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand how the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don’t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building run files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn):                 #from global dictionary of results\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    #print(scores_tuple)\n",
    "    \n",
    "    column = get_column(score_fn)\n",
    "    if column==-1:\n",
    "        print(\"Run retrieval: cannot identify score function \"+str(score_fn))\n",
    "        return\n",
    "    else:\n",
    "        print(\"Yey, column is \"+str(column))\n",
    "    run_out_path = './run_files/{}.run'.format(model_name) \n",
    "\n",
    "    if os.path.exists(run_out_path):\n",
    "        print(\"File \"+str(run_out_path)+\" already exists, operation aborted\")\n",
    "        return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "    data = {} \n",
    "    noq=0    \n",
    "    for query_id in tokenized_queries: \n",
    "        noq = noq + 1\n",
    "        temp_list = []\n",
    "        #print(\"Query \"+str(query_id)+\" no \"+str(noq)+\": got \"+str(len(rel_docs[str(query_id)]))+\" rel_docs\")\n",
    "        for rel_doc2 in rel_docs[str(query_id)]:\n",
    "            rel_doc2=int(rel_doc2)\n",
    "            #print(rel_doc2)\n",
    "            temp_list.append((scores_tuple[str(query_id), rel_doc2][column], index.document(rel_doc2)[0]))\n",
    "            data[query_id] = temp_list\n",
    "        print(\"Query no \"+str(noq)+\" is finished\")\n",
    "    \n",
    "    ##################################################\n",
    "          \n",
    "    # The dictionary data has the form: query_id --> (document_score, external_doc_id)\n",
    "    \n",
    "        \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "print(\"Number of documents: \", num_documents)\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    " \n",
    "print(\"Length of tokenized queries: \", len(tokenized_queries))\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print(\"Length of query term ids: \", len(query_term_ids))\n",
    "\n",
    "# print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        # frequency of a term in all documents\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        # frequency of a term in a particcollections.defaultdict(dict)ular document\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents\n",
    "print(\"Average document length: \", avg_doc_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# scoring functions\n",
    "\n",
    "def tfidf(int_document_id, query_term_id, document_term_freq):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \"\"\" \n",
    "    # TODO implement the function\n",
    "    \n",
    "    idf = np.log(num_documents / collection_frequencies[query_term_id])\n",
    "    tf = np.log(1 + document_term_freq)\n",
    "    score = tf*idf\n",
    "    return score\n",
    "\n",
    "\n",
    "#idf = math.log(num_documents) - math.log(df)\n",
    "\n",
    "def word2vec_similarity(query_id, doc_id, mode):\n",
    "    if mode=='avg':\n",
    "        result = 1 - spatial.distance.cosine(vec_query(query_id), vec_doc(doc_id))\n",
    "        return result\n",
    "         \n",
    "def jelinek_mercer(int_document_id, query_term_id, document_term_freq, l):\n",
    "    base = (l*document_term_freq)/document_lengths[int_document_id] \n",
    "    smooth = ((1-l)*collection_frequencies[query_term_id]/ total_terms)\n",
    "    return np.logaddexp(base, smooth)\n",
    "\n",
    "def bm25(int_document_id, query_term_id, document_term_freq):\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    ld = document_lengths[int_document_id]\n",
    "    lavg = avg_doc_length\n",
    "    tf = document_term_freq\n",
    "    df = collection_frequencies[query_term_id]\n",
    "    # +\n",
    "    idf = np.log(num_documents / collection_frequencies[query_term_id])\n",
    "    # +\n",
    "    score = (k1+1)*tf*idf/(k1*((1-b)+b*(ld/lavg))+tf)\n",
    "    return score\n",
    "\n",
    "def abs_discounting(int_document_id, query_term_id, tf, d):\n",
    "    #d = 0.1, 0.5, 0.9\n",
    "    df =  collection_frequencies[query_term_id]\n",
    "    pwc = df/doc_no  # ( tf(w;C)/|C| )\n",
    "    doc_length = document_lengths[int_document_id]\n",
    "    doc_length_unique = unique_terms_per_document[int_document_id]\n",
    "    base = np.maximum(tf-d, 0)/doc_length\n",
    "    smooth = (d*doc_length_unique/doc_length)*pwc\n",
    "    return np.logaddexp(base, smooth) \n",
    "\n",
    "def dirichlet_prior(int_document_id, query_term_id, tf, mu):\n",
    "#     base = (document_term_freq)/(document_lengths[int_document_id] + mu)\n",
    "#     smooth = (mu*collection_frequencies[query_term_id])/(document_lengths[int_document_id] + mu) \n",
    "    df = collection_frequencies[query_term_id]\n",
    "    pwc = df/doc_no\n",
    "    #collection_frequencies[query_term_id] instead of pwc\n",
    "    doc_length = document_lengths[int_document_id] \n",
    "    base = ((doc_length)/(doc_length + mu))*(tf/doc_length)\n",
    "    smooth = (mu*(pwc))/(doc_length + mu)\n",
    "    return np.logaddexp(base, smooth) \n",
    "\n",
    "def vec_query_lsm(query_id, model):\n",
    "    query_tokens = [id2token[int(word_id)] for word_id in tokenized_queries[str(query_id)] if word_id > 0]\n",
    "    query_bow = dictionary.doc2bow(query_tokens)\n",
    "    # infer topic distributions on new, unseen documents\n",
    "    query_vec = [x[1] for x in model[query_bow]]\n",
    "    return query_vec\n",
    "\n",
    "def vec_doc_lsm(int_document_id, model):\n",
    "    doc_tokens = [word_id for word_id in index.document(int(int_document_id))[1] if word_id > 0]\n",
    "    # infer topic distributions on new, unseen documents\n",
    "    doc_bow = dictionary.doc2bow(doc_tokens)\n",
    "    doc_vec = [x[1] for x in model[doc_bow]]\n",
    "    return doc_vec\n",
    "\n",
    "def lsi(int_document_id, query_id):\n",
    "    result = 1 - spatial.distance.cosine(vec_query_lsm(int(query_id), lsi_model), vec_doc_lsm(int_document_id, lsi_model))\n",
    "    return result\n",
    "\n",
    "def lda(int_document_id, query_id):\n",
    "    result = 1 - spatial.distance.cosine(vec_query_lsm(int(query_id), lda_model), vec_doc_lsm(int_document_id, lda_model))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part 1 (scores tuple)\n",
    "\n",
    "mode='avg'\n",
    "no=0\n",
    "scores_tuple = collections.defaultdict(dict)\n",
    "rel_docs = collections.defaultdict(dict)\n",
    "\n",
    "for query_id in tokenized_queries: \n",
    "    no+=1\n",
    "    print(\"Query \"+str(no)+\" of 150\")\n",
    "    temp_rel_docs = set()\n",
    "    for query_term_id in tokenized_queries[query_id]:\n",
    "        part2 = inverted_index[query_term_id]\n",
    "        for rel_doc in part2:\n",
    "            temp_rel_docs.add(str(rel_doc))\n",
    "            tf = inverted_index[query_term_id][rel_doc]\n",
    "            if not (scores_tuple[query_id, rel_doc]):                #empty list - not yet initialized\n",
    "                scores_tuple[query_id, rel_doc] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   #13 dims     \n",
    "            scores_tuple[query_id, rel_doc][0] += tfidf(rel_doc, query_term_id, tf)\n",
    "            scores_tuple[query_id, rel_doc][1] += dirichlet_prior(rel_doc, query_term_id, tf, mu=500)\n",
    "            scores_tuple[query_id, rel_doc][2] += dirichlet_prior(rel_doc, query_term_id, tf, mu=1000)\n",
    "            scores_tuple[query_id, rel_doc][3] += dirichlet_prior(rel_doc, query_term_id, tf, mu=1500)\n",
    "            scores_tuple[query_id, rel_doc][4] += bm25(rel_doc, query_term_id, tf)\n",
    "            scores_tuple[query_id, rel_doc][5] += abs_discounting(rel_doc, query_term_id, tf, d=0.1)\n",
    "            scores_tuple[query_id, rel_doc][6] += abs_discounting(rel_doc, query_term_id, tf, d=0.5)\n",
    "            scores_tuple[query_id, rel_doc][7] += abs_discounting(rel_doc, query_term_id, tf, d=0.9)\n",
    "            scores_tuple[query_id, rel_doc][8] += jelinek_mercer(rel_doc, query_term_id, tf, l=0.1)\n",
    "            scores_tuple[query_id, rel_doc][9] += jelinek_mercer(rel_doc, query_term_id, tf, l=0.5)\n",
    "            scores_tuple[query_id, rel_doc][10] += jelinek_mercer(rel_doc, query_term_id, tf, l=0.9)\n",
    "    rel_docs[query_id] = temp_rel_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no=0\n",
    "for query_id in tokenized_queries: \n",
    "    no+=1\n",
    "    print(\"Query (\"+str(query_id)+\"), no\"+str(no)+\" of 150\")\n",
    "    for rel_doc in rel_docs[query_id]:\n",
    "        #print(\"Rel_doc \"+str(rel_doc))\n",
    "        rel_doc = int(rel_doc)\n",
    "        scores_tuple[query_id, rel_doc][11] = lsi(rel_doc, query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for query_id in tokenized_queries: \n",
    "    no+=1\n",
    "    print(\"Query (\"+str(query_id)+\"), no\"+str(no)+\" of 150\")\n",
    "    for rel_doc in rel_docs[query_id]:\n",
    "        rel_doc = int(rel_doc)\n",
    "        scores_tuple2[query_id, rel_doc][12] = word2vec_similarity(query_id, rel_doc, \"avg\")\n",
    "        #scores_tuple2[query_id, rel_doc][0] = lsi_similarity(query_id, rel_doc, \"avg\")\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_column(scorefn_name):\n",
    "    if scorefn_name==\"tfidf\":\n",
    "        print(\"get_col: tfidf\")\n",
    "        return 0\n",
    "    elif scorefn_name==\"d500\":\n",
    "        print(\"get_col: d500\")\n",
    "        return 1\n",
    "    elif scorefn_name==\"d1000\":\n",
    "        print(\"get_col: d1000\")\n",
    "        return 2\n",
    "    elif scorefn_name==\"d1500\":\n",
    "        print(\"get_col: d1500\")\n",
    "        return 3\n",
    "    elif scorefn_name==\"bm25\":\n",
    "        print(\"get_col: bm25\")\n",
    "        return 4\n",
    "    elif scorefn_name==\"abs01\":\n",
    "        print(\"get_col: abs01\")\n",
    "        return 5\n",
    "    elif scorefn_name==\"abs05\":\n",
    "        print(\"get_col: abs05\")\n",
    "        return 6\n",
    "    elif scorefn_name==\"abs09\":\n",
    "        print(\"get_col: abs09\")\n",
    "        return 7 \n",
    "    elif scorefn_name==\"jm01\":\n",
    "        print(\"get_col: jm01\")\n",
    "        return 8\n",
    "    elif scorefn_name==\"jm05\":\n",
    "        print(\"get_col: jm05\")\n",
    "        return 9\n",
    "    elif scorefn_name==\"jm09\":\n",
    "        print(\"get_col: jm09\")\n",
    "        return 10\n",
    "    elif scorefn_name==\"lsi\":\n",
    "        print(\"get_col: lsi\")\n",
    "        return 11\n",
    "    elif scorefn_name==\"w2vavg\":\n",
    "        print(\"get_col: w2vavg\")\n",
    "        return 12\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_retrieval('lsi','lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query (199), 1/150 is finished\n",
      "Query (102), 2/150 is finished\n",
      "Query (69), 3/150 is finished\n",
      "Query (106), 4/150 is finished\n",
      "Query (79), 5/150 is finished\n",
      "Query (180), 6/150 is finished\n",
      "Query (52), 7/150 is finished\n",
      "Query (168), 8/150 is finished\n",
      "Query (158), 9/150 is finished\n",
      "Query (85), 10/150 is finished\n",
      "Query (107), 11/150 is finished\n",
      "Query (155), 12/150 is finished\n",
      "Query (59), 13/150 is finished\n",
      "Query (129), 14/150 is finished\n",
      "Query (61), 15/150 is finished\n",
      "Query (182), 16/150 is finished\n",
      "Query (190), 17/150 is finished\n",
      "Query (53), 18/150 is finished\n",
      "Query (132), 19/150 is finished\n",
      "Query (186), 20/150 is finished\n",
      "Query (76), 21/150 is finished\n",
      "Query (87), 22/150 is finished\n",
      "Query (89), 23/150 is finished\n",
      "Query (174), 24/150 is finished\n",
      "Query (141), 25/150 is finished\n",
      "Query (77), 26/150 is finished\n",
      "Query (156), 27/150 is finished\n",
      "Query (78), 28/150 is finished\n",
      "Query (96), 29/150 is finished\n",
      "Query (60), 30/150 is finished\n",
      "Query (116), 31/150 is finished\n",
      "Query (63), 32/150 is finished\n",
      "Query (133), 33/150 is finished\n",
      "Query (73), 34/150 is finished\n",
      "Query (177), 35/150 is finished\n",
      "Query (192), 36/150 is finished\n",
      "Query (142), 37/150 is finished\n",
      "Query (185), 38/150 is finished\n",
      "Query (169), 39/150 is finished\n",
      "Query (193), 40/150 is finished\n",
      "Query (184), 41/150 is finished\n",
      "Query (115), 42/150 is finished\n",
      "Query (68), 43/150 is finished\n",
      "Query (88), 44/150 is finished\n",
      "Query (198), 45/150 is finished\n",
      "Query (94), 46/150 is finished\n",
      "Query (171), 47/150 is finished\n",
      "Query (81), 48/150 is finished\n",
      "Query (136), 49/150 is finished\n",
      "Query (138), 50/150 is finished\n",
      "Query (112), 51/150 is finished\n",
      "Query (161), 52/150 is finished\n",
      "Query (86), 53/150 is finished\n",
      "Query (121), 54/150 is finished\n",
      "Query (62), 55/150 is finished\n",
      "Query (154), 56/150 is finished\n",
      "Query (165), 57/150 is finished\n",
      "Query (105), 58/150 is finished\n",
      "Query (99), 59/150 is finished\n",
      "Query (103), 60/150 is finished\n",
      "Query (157), 61/150 is finished\n",
      "Query (72), 62/150 is finished\n",
      "Query (130), 63/150 is finished\n",
      "Query (196), 64/150 is finished\n",
      "Query (118), 65/150 is finished\n",
      "Query (151), 66/150 is finished\n",
      "Query (175), 67/150 is finished\n",
      "Query (71), 68/150 is finished\n",
      "Query (200), 69/150 is finished\n",
      "Query (104), 70/150 is finished\n",
      "Query (144), 71/150 is finished\n",
      "Query (120), 72/150 is finished\n",
      "Query (97), 73/150 is finished\n",
      "Query (124), 74/150 is finished\n",
      "Query (119), 75/150 is finished\n",
      "Query (123), 76/150 is finished\n",
      "Query (92), 77/150 is finished\n",
      "Query (135), 78/150 is finished\n",
      "Query (159), 79/150 is finished\n",
      "Query (111), 80/150 is finished\n",
      "Query (82), 81/150 is finished\n",
      "Query (57), 82/150 is finished\n",
      "Query (191), 83/150 is finished\n",
      "Query (74), 84/150 is finished\n",
      "Query (128), 85/150 is finished\n",
      "Query (176), 86/150 is finished\n",
      "Query (110), 87/150 is finished\n",
      "Query (65), 88/150 is finished\n",
      "Query (194), 89/150 is finished\n",
      "Query (84), 90/150 is finished\n",
      "Query (125), 91/150 is finished\n",
      "Query (100), 92/150 is finished\n",
      "Query (178), 93/150 is finished\n",
      "Query (54), 94/150 is finished\n",
      "Query (134), 95/150 is finished\n",
      "Query (56), 96/150 is finished\n",
      "Query (75), 97/150 is finished\n",
      "Query (117), 98/150 is finished\n",
      "Query (83), 99/150 is finished\n",
      "Query (143), 100/150 is finished\n",
      "Query (197), 101/150 is finished\n",
      "Query (167), 102/150 is finished\n",
      "Query (108), 103/150 is finished\n",
      "Query (55), 104/150 is finished\n",
      "Query (64), 105/150 is finished\n",
      "Query (181), 106/150 is finished\n",
      "Query (160), 107/150 is finished\n",
      "Query (146), 108/150 is finished\n",
      "Query (91), 109/150 is finished\n",
      "Query (126), 110/150 is finished\n",
      "Query (163), 111/150 is finished\n",
      "Query (187), 112/150 is finished\n",
      "Query (114), 113/150 is finished\n",
      "Query (90), 114/150 is finished\n",
      "Query (98), 115/150 is finished\n",
      "Query (153), 116/150 is finished\n",
      "Query (195), 117/150 is finished\n",
      "Query (172), 118/150 is finished\n",
      "Query (109), 119/150 is finished\n",
      "Query (127), 120/150 is finished\n",
      "Query (164), 121/150 is finished\n",
      "Query (131), 122/150 is finished\n",
      "Query (162), 123/150 is finished\n",
      "Query (93), 124/150 is finished\n",
      "Query (139), 125/150 is finished\n",
      "Query (152), 126/150 is finished\n",
      "Query (179), 127/150 is finished\n",
      "Query (66), 128/150 is finished\n",
      "Query (189), 129/150 is finished\n",
      "Query (188), 130/150 is finished\n",
      "Query (149), 131/150 is finished\n",
      "Query (95), 132/150 is finished\n",
      "Query (58), 133/150 is finished\n",
      "Query (170), 134/150 is finished\n",
      "Query (147), 135/150 is finished\n",
      "Query (140), 136/150 is finished\n",
      "Query (166), 137/150 is finished\n",
      "Query (67), 138/150 is finished\n",
      "Query (101), 139/150 is finished\n",
      "Query (70), 140/150 is finished\n",
      "Query (148), 141/150 is finished\n",
      "Query (80), 142/150 is finished\n",
      "Query (113), 143/150 is finished\n",
      "Query (173), 144/150 is finished\n",
      "Query (150), 145/150 is finished\n",
      "Query (145), 146/150 is finished\n",
      "Query (122), 147/150 is finished\n",
      "Query (137), 148/150 is finished\n",
      "Query (183), 149/150 is finished\n",
      "Query (51), 150/150 is finished\n",
      "Inverted index creation took 334.6478033065796 seconds.\n"
     ]
    }
   ],
   "source": [
    "# part 2 (limit to 1000 docs)\n",
    "\n",
    "retrieval_start_time = time.time()\n",
    "\n",
    "max_objects_per_query = 1000\n",
    "data2 = {}\n",
    "noq=0    \n",
    "\n",
    "for query_id in tokenized_queries: \n",
    "    noq = noq + 1\n",
    "    temp_list = []\n",
    "    #print(\"Query \"+str(query_id)+\" no \"+str(noq)+\": got \"+str(len(rel_docs[str(query_id)]))+\" rel_docs\")\n",
    "    for rel_doc2 in rel_docs[str(query_id)]:\n",
    "        rel_doc2=int(rel_doc2)\n",
    "        #print(rel_doc2)\n",
    "        #temp_list.append((scores_tuple[str(query_id), rel_doc2][0], index.document(rel_doc2)[0]))\n",
    "        temp_list.append((scores_tuple[str(query_id), rel_doc2][0], index.document(rel_doc2)[0]))\n",
    "        data2[query_id] = temp_list\n",
    "\n",
    "\n",
    "    for subject_id, object_assesments in data2.items ():\n",
    "        #print('Subject_id is '+str(subject_id)+\" and object_assessments is \"+str(obmax_objects_per_queryject_assesments))\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "            data2[query_id] = object_assesments\n",
    "    print(\"Query (\"+str(query_id)+\"), \"+str(noq)+\"/150 is finished\")\n",
    "    \n",
    "print('Data2 creation took', time.time() - retrieval_start_time, 'seconds.')\n",
    "\n",
    "#proof : len(data2['186']) -> is always gonna be 1000\n",
    "#also : if u check 'object_assesments' object of each query you'll see it only keeps the highest scores.\n",
    "\n",
    "#So basically your data is in data2 dictionary:\n",
    "# data2[query_id] = [(doc1,tfidfscore1), (doc2, tfidfscore2), ... ,(doc1000, tfidfscore1000)]\n",
    "# data2[query_id] object is a list, and query_id should be provided as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data2 in a (q_id, int_doc_id) -> score\n",
    "\n",
    "q_doc_tfidfs = collections.defaultdict(dict)\n",
    "\n",
    "\n",
    "for query_id in tokenized_queries:\n",
    "    for tuple in data2[query_id]:\n",
    "        q_doc_tfidfs[(query_id, ext_id_mapping[tuple[1]])] = tuple[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_doc_tfidfs[('100', 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3259464411605346, 4.5545345241203723)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dict = {}\n",
    "\n",
    "scores_dict['tfidf'] = [v[0] for k, v in scores_tuple.items()]\n",
    "\n",
    "scores_dict['bm25']  = [v[1] for k, v in scores_tuple.items()]\n",
    "\n",
    "scores_dict['dirichlet500'] = [v[2] for k, v in scores_tuple.items()]\n",
    "scores_dict['dirichlet1000'] = [v[3] for k, v in scores_tuple.items()]\n",
    "scores_dict['dirichlet1500'] = [v[4] for k, v in scores_tuple.items()]\n",
    "\n",
    "# scores_dict['jelinek01'] = [v[5] for k, v in scores_tuple.items()]\n",
    "# scores_dict['jelinek05'] = [v[6] for k, v in scores_tuple.items()]\n",
    "# scores_dict['jelinek09'] = [v[7] for k, v in scores_tuple.items()]\n",
    "\n",
    "# scores_dict['abs_discounting01'] = [v[8] for k, v in scores_tuple.items()]\n",
    "# scores_dict['abs_discounting05'] = [v[9] for k, v in scores_tuple.items()]\n",
    "# scores_dict['abs_discounting09'] = [v[10] for k, v in scores_tuple.items()]\n",
    "\n",
    "\n",
    "scores_dict['abs_discounting01'] = [v[5] for k, v in scores_tuple.items()]\n",
    "scores_dict['abs_discounting05'] = [v[6] for k, v in scores_tuple.items()]\n",
    "scores_dict['abs_discounting09'] = [v[7] for k, v in scores_tuple.items()]\n",
    "\n",
    "\n",
    "scores_dict['tfidf'][5], scores_dict['bm25'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation: Which parameters do include (determine with t-tests on MAP per query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scores = pd.DataFrame()\n",
    "scores_val = {}\n",
    "# score_names = ['d1000', 'dtfidf', 'abs09', 'd1500', 'bm25', 'abs01', 'abs05', 'd500']\n",
    "\n",
    "# TODO: JM\n",
    "score_names = ['d500', 'd1000', 'd1500', 'abs01', 'abs05', 'abs09']\n",
    "        \n",
    "for score in score_names:\n",
    "    score_file = 'scores/val/' + score + '.txt'\n",
    "#     if score == 'd1000':\n",
    "#         scores['query_id'] = pd.read_csv(score_file, sep='\\t', header=None)[1]\n",
    "    scores_val[score] = pd.read_csv(score_file, sep='\\t', header=None)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0299\n",
       "1    0.0484\n",
       "2    0.0032\n",
       "3    0.0396\n",
       "4    0.0004\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_val['abs01'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No statistical significance when comparing different hyperparameter value for retrieval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['d1000', 'd500', 'd1500', 'abs01', 'abs09', 'abs05'])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models_compared</th>\n",
       "      <th>p-value</th>\n",
       "      <th>p_values_corr</th>\n",
       "      <th>h0_rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d500_abs01</td>\n",
       "      <td>0.022120</td>\n",
       "      <td>0.331806</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d500_abs05</td>\n",
       "      <td>0.022212</td>\n",
       "      <td>0.333178</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d500_abs09</td>\n",
       "      <td>0.022218</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d1000_abs01</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.412707</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d1000_abs05</td>\n",
       "      <td>0.027626</td>\n",
       "      <td>0.414383</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d1000_abs09</td>\n",
       "      <td>0.027634</td>\n",
       "      <td>0.414508</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>d1500_abs01</td>\n",
       "      <td>0.030619</td>\n",
       "      <td>0.459283</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d1500_abs05</td>\n",
       "      <td>0.030743</td>\n",
       "      <td>0.461139</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d1500_abs09</td>\n",
       "      <td>0.030753</td>\n",
       "      <td>0.461292</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>d1000_d500</td>\n",
       "      <td>0.060381</td>\n",
       "      <td>0.905713</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abs01_abs05</td>\n",
       "      <td>0.290986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>abs01_abs09</td>\n",
       "      <td>0.243349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>abs09_abs05</td>\n",
       "      <td>0.572226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>d1000_d1500</td>\n",
       "      <td>0.124692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>d500_d1500</td>\n",
       "      <td>0.074184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   models_compared   p-value  p_values_corr  h0_rejected\n",
       "0       d500_abs01  0.022120       0.331806        False\n",
       "1       d500_abs05  0.022212       0.333178        False\n",
       "2       d500_abs09  0.022218       0.333276        False\n",
       "3      d1000_abs01  0.027514       0.412707        False\n",
       "4      d1000_abs05  0.027626       0.414383        False\n",
       "5      d1000_abs09  0.027634       0.414508        False\n",
       "6      d1500_abs01  0.030619       0.459283        False\n",
       "7      d1500_abs05  0.030743       0.461139        False\n",
       "8      d1500_abs09  0.030753       0.461292        False\n",
       "9       d1000_d500  0.060381       0.905713        False\n",
       "10     abs01_abs05  0.290986       1.000000        False\n",
       "11     abs01_abs09  0.243349       1.000000        False\n",
       "12     abs09_abs05  0.572226       1.000000        False\n",
       "13     d1000_d1500  0.124692       1.000000        False\n",
       "14      d500_d1500  0.074184       1.000000        False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/29561360/how-to-calculate-p-value-for-two-lists-of-floats\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "p_values = {}\n",
    "\n",
    "\n",
    "# scores_dict.keys()\n",
    "for list1, list2 in combinations(scores_val.keys(), 2):\n",
    "    t, p = stats.ttest_rel(scores_val[list1], scores_val[list2])\n",
    "    \n",
    "    comp_name = str(list1) + \"_\" + str(list2)\n",
    "    p_values[comp_name] = [p]\n",
    "    # print(list1, list2, p)\n",
    "    \n",
    "\n",
    "p_values = pd.DataFrame(data=p_values).T.reset_index()\n",
    "p_values.columns = ['models_compared', 'p-value']\n",
    "\n",
    "\n",
    "p_values_corr = multipletests(list(p_values['p-value']), alpha=0.05, method='bonferroni')\n",
    "p_values['p_values_corr'] = p_values_corr[1]\n",
    "p_values['h0_rejected'] = p_values_corr[0]\n",
    "p_values = p_values.sort_values(by='p_values_corr', ascending=True)\n",
    "p_values.reset_index(inplace=True, drop=True)\n",
    "p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As there is no statistical significance in different hyper-parameter values we decide to use the ones with the lowest values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1497903225806452, 0.14731612903225808, 0.145941935483871)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_val['d500'].mean(), scores_val['d1000'].mean(), scores_val['d1500'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09226774193548386, 0.09230967741935484, 0.09231612903225805)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_val['abs01'].mean(), scores_val['abs05'].mean(), scores_val['abs09'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: JM\n",
    "# scores['jm01'].mean(), scores['jm05'].mean(), scores['jm09'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD/CAYAAAAKVJb/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f3ff77860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_val['abs05'].plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD/CAYAAAAKVJb/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f3f9fb630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_val['d500'].plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting scores (NDCG@10, MAP@1000, Precision@5, Recall@1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scores = pd.DataFrame()\n",
    "scores_test = pd.DataFrame()\n",
    "# TODO: JM, LSI, LDA, word2vec\n",
    "score_names = ['tfidf', 'bm25', 'abs01', 'd500']\n",
    "        \n",
    "for score in score_names:\n",
    "    score_file = 'scores/test/' + score + '_test.txt'\n",
    "    if score == 'tfidf':\n",
    "        scores_test['score'] = pd.read_csv(score_file, sep='\\t', header=None)[0]\n",
    "        scores_test['query_id'] = pd.read_csv(score_file, sep='\\t', header=None)[1]\n",
    "    scores_test[score] = pd.read_csv(score_file, sep='\\t', header=None)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['map                   ',\n",
       " 'P_5                   ',\n",
       " 'P_500                 ',\n",
       " 'recall_1000           ',\n",
       " 'ndcg_cut_10           ',\n",
       " 'ndcg_cut_100          ',\n",
       " 'ndcg_cut_1000         ',\n",
       " 'map_cut_1000          ',\n",
       " 'relative_P_5          ',\n",
       " 'relative_P_500        ']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(scores_test['score'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['map',\n",
       " 'P_5',\n",
       " 'P_500',\n",
       " 'recall_1000',\n",
       " 'ndcg_cut_10',\n",
       " 'ndcg_cut_100',\n",
       " 'ndcg_cut_1000',\n",
       " 'map_cut_1000',\n",
       " 'relative_P_5',\n",
       " 'relative_P_500']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_test['score'] = pd.Series([score .strip() for score in scores_test['score']])\n",
    "list(scores_test['score'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoring_metrics = ['ndcg_cut_10', 'map', 'P_5', 'recall_1000']\n",
    "test_metrics_df = pd.DataFrame()\n",
    "\n",
    "def get_avg_score(metric):\n",
    "    df = scores_test[(scores_test['score'] == metric) &  (scores_test['query_id'] == 'all')]\n",
    "    return df\n",
    "\n",
    "for scoring_metric in scoring_metrics:\n",
    "    test_metrics_df = test_metrics_df.append(get_avg_score(scoring_metric), ignore_index=True)\n",
    "    # print(get_avg_score(scoring_metric))\n",
    "    \n",
    "test_metrics_df = test_metrics_df.drop('query_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>bm25</th>\n",
       "      <th>abs01</th>\n",
       "      <th>d500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ndcg_cut_10</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>0.1636</td>\n",
       "      <td>0.2479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>map</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>0.1319</td>\n",
       "      <td>0.0683</td>\n",
       "      <td>0.1137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_5</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.3467</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.2450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recall_1000</td>\n",
       "      <td>0.6432</td>\n",
       "      <td>0.4616</td>\n",
       "      <td>0.3689</td>\n",
       "      <td>0.4617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         score   tfidf    bm25   abs01    d500\n",
       "0  ndcg_cut_10  0.4000  0.3257  0.1636  0.2479\n",
       "1          map  0.2114  0.1319  0.0683  0.1137\n",
       "2          P_5  0.4083  0.3467  0.1667  0.2450\n",
       "3  recall_1000  0.6432  0.4616  0.3689  0.4617"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD5CAYAAADcDXXiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYCDCHQAMRLgDgIH+Dx+yqg+p0H0TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f3f716710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_metrics_df[test_metrics_df['score'] == 'ndcg_cut_10'][list(test_metrics_df.columns)[1:]].plot(kind='bar', ylim=(0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [20 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods.\n",
    "\n",
    "Perform analysis **[5 points]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pyndri.compat\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IndriCorpus(gensim.interfaces.CorpusABC):\n",
    "\n",
    "    def __init__(self, index, dictionary, max_documents=None):\n",
    "        assert isinstance(index, pyndri.Index)\n",
    "\n",
    "        self.index = index\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "        self.max_documents = max_documents\n",
    "\n",
    "    def _maximum_document(self):\n",
    "        if self.max_documents is None:\n",
    "            return self.index.maximum_document()\n",
    "        else:\n",
    "            return min(\n",
    "                self.max_documents + self.index.document_base(),\n",
    "                self.index.maximum_document())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for int_doc_id in range(self.index.document_base(),\n",
    "                                self._maximum_document()):\n",
    "            ext_doc_id, tokens = self.index.document(int_doc_id)\n",
    "\n",
    "            # Compared to IndriSentences, the only difference is the\n",
    "            # switching of tuple(self.dictionary[token_id] ...) by\n",
    "            # sorted(collections.Counter(token_id ...).items()).\n",
    "            yield sorted(collections.Counter(\n",
    "                token_id\n",
    "                for token_id in tokens\n",
    "                if token_id > 0 and token_id in self.dictionary).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# docs 1-1000\n",
    "# index_models = [word_id for word_id in index.document(int(int_document_id))[1] if word_id > 0]\n",
    "index_models = [index.document(int_doc) for int_doc in range(index.document_base(), index.document_base()+1000)]\n",
    "# dictionary_models = pyndri.extract_dictionary(index_models)\n",
    "# corpus_models = IndriCorpus(index_models, dictionary_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = IndriCorpus(index, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_model = models.LsiModel(corpus=corpus, id2word=id2token, num_topics=20)\n",
    "lsi_model.save('/tmp/model.lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_model = models.LsiModel.load('/tmp/model.lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mm = corpora.MmCorpus('corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-df43a282973b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/model.lda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus_models' is not defined"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus_models, id2word=id2token, num_topics=20, update_every=0, passes=20)\n",
    "lda_model.save('/tmp/model.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = models.LsiModel.load('/tmp/model.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "texts = []\n",
    "for i in range (index.document_base(),index.maximum_document()):\n",
    "    # texts[i-1] = ([id2token[word_id] for word_id in index.document(i)[1] if word_id > 0])\n",
    "    texts.append([id2token[word_id] for word_id in index.document(i)[1] if word_id > 0])\n",
    "\n",
    "len(texts)\n",
    "\n",
    "# dictionary_corpus = corpora.Dictionary(texts)\n",
    "# type(dictionary_corpus)\n",
    "\n",
    "# get doc bow out of tokenized docs\n",
    "# corpus_docs = [dictionary_corpus.doc2bow(text) for text in texts]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# query\n",
    "tokenized_queries['100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_tokens = [id2token[word_id] for word_id in tokenized_queries['100'] if word_id > 0]\n",
    "query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_bow = dictionary.doc2bow(query_tokens)\n",
    "query_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_vec = [x[1] for x in lsi[query_bow]]\n",
    "query_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def vec_query_lsm(query_id, model):\n",
    "    query_tokens = [id2token[word_id] for word_id in tokenized_queries[str(query_id)] if word_id > 0]\n",
    "    query_bow = dictionary.doc2bow(query_tokens)\n",
    "    # infer topic distributions on new, unseen documents\n",
    "    query_vec = [x[1] for x in model[query_bow]]\n",
    "    return query_vec\n",
    "\n",
    "def vec_doc_lsm(int_document_id, model):\n",
    "    doc_tokens = [word_id for word_id in index.document(int(int_document_id))[1] if word_id > 0]\n",
    "    # infer topic distributions on new, unseen documents\n",
    "    doc_bow = dictionary.doc2bow(doc_tokens)\n",
    "    doc_vec = [x[1] for x in model[doc_bow]]\n",
    "    return doc_vec\n",
    "\n",
    "def lsi(query_id, int_document_id):\n",
    "    result = 1 - spatial.distance.cosine(vec_query_lsm(query_id, lsi_model), vec_doc_lsm(int_document_id, lsi_model))\n",
    "    return result\n",
    "\n",
    "def lda(query_id, int_document_id):\n",
    "    result = 1 - spatial.distance.cosine(vec_query_lsm(query_id, lda_model), vec_doc_lsm(int_document_id, lda_model))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi('100', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda('100', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.print_topics(num_topics=5, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text Analytics - Latent Semantic Analysis - https://www.youtube.com/watch?v=BJ0MnawUpaU\n",
    "# https://radimrehurek.com/gensim/models/lsimodel.html\n",
    "# https://radimrehurek.com/gensim/wiki.html\n",
    "# Latent Semantic Analysis in Python - http://blog.josephwilk.net/projects/latent-semantic-analysis-in-python.html\n",
    "# Building a Vector Space Search Engine in Python - http://blog.josephwilk.net/projects/building-a-vector-space-search-engine-in-python.html\n",
    "# https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py\n",
    "# integrate gensim and pyndri\n",
    "# https://radimrehurek.com/gensim/tut2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task 3:  Word embeddings for ranking [10 points] ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "\n",
    "Try one of the following (increasingly complex) methods for building query and document representations:\n",
    "   * Average or sum the word vectors.\n",
    "   * Cluster words in the document using [k-means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and use the centroid of the most important cluster. Experiment with different values of K for k-means.\n",
    "   * Using the [bag-of-word-embeddings representation](https://ciir-publications.cs.umass.edu/pub/web/getpdf.php?id=1248).\n",
    "   \n",
    "Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.info('Initializing word2vec.')\n",
    "\n",
    "word2vec_init = gensim.models.Word2Vec(\n",
    "    size=20,  # Embedding size\n",
    "    window=5,  # One-sided window size\n",
    "    sg=True,  # Skip-gram.\n",
    "    min_count=5,  # Minimum word frequency.\n",
    "    sample=1e-3,  # Sub-sample threshold.\n",
    "    hs=False,  # Hierarchical softmax.\n",
    "    negative=10,  # Number of negative examples.\n",
    "    iter=1,  # Number of iterations.\n",
    "    workers=8,  # Number of workers.\n",
    ")\n",
    "with pyndri.open('index/') as index:\n",
    "    logging.info('Loading vocabulary.')\n",
    "    dictionary = pyndri.extract_dictionary(index)\n",
    "    sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "    logging.info('Constructing word2vec vocabulary.')\n",
    "\n",
    "    # Build vocab.\n",
    "    word2vec_init.build_vocab(sentences, trim_rule=None)\n",
    "\n",
    "    model = word2vec_init\n",
    "    \n",
    "    model.train(sentences,total_examples=model.corpus_count, epochs=2) \n",
    "\n",
    "    model.save('model_20')\n",
    "\n",
    "logging.info('Trained models: %s', models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec.load('./word2vec_model/model_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_query(query_id):\n",
    "    query_vec = []\n",
    "    for query_term_id in tokenized_queries[query_id]:\n",
    "        term = id2token[query_term_id]\n",
    "        try:\n",
    "            vec = w2v_model[str(term)]\n",
    "            query_vec.append(vec)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    query_avg = np.mean(np.array(query_vec), axis = 0)\n",
    "    \n",
    "    return (query_avg)\n",
    "\n",
    "def vec_doc(doc_id):\n",
    "    doc_vec = []\n",
    "    tokens = [id2token[word_id] for word_id in index.document(int(doc_id))[1] if word_id > 0]\n",
    "    for term in tokens:\n",
    "        try:\n",
    "            vecs = w2v_model[str(term)]\n",
    "            doc_vec.append(vecs)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    doc_avg = np.mean(np.array(doc_vec), axis = 0)\n",
    "    \n",
    "    return (doc_avg)\n",
    "\n",
    "\n",
    "def word2vec_similarity(query_id, doc_id, mode):\n",
    "    if mode=='avg':\n",
    "        result = 1 - spatial.distance.cosine(vec_query(query_id), vec_doc(doc_id))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task 4: Learning to rank (LTR) [10 points] ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval, in particular pointwise learning to rank.\n",
    "\n",
    "You will experiment with a pointwise learning to rank method, logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1 and Task 2 as features. Think about other features you can use (e.g. query/document length). \n",
    "One idea is to also explore external sources such as Wikipedia entities (?). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "how to get top 1000 docs per query?\n",
    "('100, 1)\n",
    "150.000 unique combinations\n",
    "\n",
    "1. get unique combinations in a tuple (query-doc pair)\n",
    "2. extract features from the scores_tuple\n",
    "3. extract the y values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_tuple['100', 2]\n",
    "len(scores_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirichlet500</th>\n",
       "      <th>abs_discounting05</th>\n",
       "      <th>dirichlet1000</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>abs_discounting01</th>\n",
       "      <th>abs_discounting09</th>\n",
       "      <th>bm25</th>\n",
       "      <th>dirichlet1500</th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.705806</td>\n",
       "      <td>5.470864</td>\n",
       "      <td>4.737921</td>\n",
       "      <td>1.027363</td>\n",
       "      <td>5.470418</td>\n",
       "      <td>5.471310</td>\n",
       "      <td>4.621646</td>\n",
       "      <td>0.301530</td>\n",
       "      <td>101</td>\n",
       "      <td>109068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.351107</td>\n",
       "      <td>4.221463</td>\n",
       "      <td>4.401678</td>\n",
       "      <td>1.103728</td>\n",
       "      <td>4.220640</td>\n",
       "      <td>4.222286</td>\n",
       "      <td>4.227335</td>\n",
       "      <td>-0.319026</td>\n",
       "      <td>126</td>\n",
       "      <td>135797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.979879</td>\n",
       "      <td>6.578906</td>\n",
       "      <td>4.999527</td>\n",
       "      <td>0.278848</td>\n",
       "      <td>6.578601</td>\n",
       "      <td>6.579211</td>\n",
       "      <td>4.925765</td>\n",
       "      <td>0.180339</td>\n",
       "      <td>73</td>\n",
       "      <td>56854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.904076</td>\n",
       "      <td>3.381663</td>\n",
       "      <td>3.945469</td>\n",
       "      <td>1.873809</td>\n",
       "      <td>3.381128</td>\n",
       "      <td>3.382197</td>\n",
       "      <td>3.799341</td>\n",
       "      <td>-0.173230</td>\n",
       "      <td>164</td>\n",
       "      <td>26099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.336076</td>\n",
       "      <td>6.916400</td>\n",
       "      <td>5.368383</td>\n",
       "      <td>-0.568725</td>\n",
       "      <td>6.915922</td>\n",
       "      <td>6.916876</td>\n",
       "      <td>5.251475</td>\n",
       "      <td>0.299376</td>\n",
       "      <td>113</td>\n",
       "      <td>13592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dirichlet500  abs_discounting05  dirichlet1000     tfidf  \\\n",
       "0      4.705806           5.470864       4.737921  1.027363   \n",
       "1      4.351107           4.221463       4.401678  1.103728   \n",
       "2      4.979879           6.578906       4.999527  0.278848   \n",
       "3      3.904076           3.381663       3.945469  1.873809   \n",
       "4      5.336076           6.916400       5.368383 -0.568725   \n",
       "\n",
       "   abs_discounting01  abs_discounting09      bm25  dirichlet1500 query_id  \\\n",
       "0           5.470418           5.471310  4.621646       0.301530      101   \n",
       "1           4.220640           4.222286  4.227335      -0.319026      126   \n",
       "2           6.578601           6.579211  4.925765       0.180339       73   \n",
       "3           3.381128           3.382197  3.799341      -0.173230      164   \n",
       "4           6.915922           6.916876  5.251475       0.299376      113   \n",
       "\n",
       "   doc_id  \n",
       "0  109068  \n",
       "1  135797  \n",
       "2   56854  \n",
       "3   26099  \n",
       "4   13592  "
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame()\n",
    "for feature in [str(key) for key in scores_dict.keys()]:\n",
    "    X[feature] = scores_dict[feature]\n",
    "\n",
    "query_doc_pairs = list(scores_tuple.keys())\n",
    "X['query_id'] = [q_doc_p[0] for q_doc_p in query_doc_pairs]\n",
    "X['doc_id'] = [q_doc_p[1] for q_doc_p in query_doc_pairs]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ext_id</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP890520-0085</td>\n",
       "      <td>82086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP891002-0194</td>\n",
       "      <td>39994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP880926-0171</td>\n",
       "      <td>161630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP890913-0206</td>\n",
       "      <td>72480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP880315-0009</td>\n",
       "      <td>113025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ext_id  doc_id\n",
       "0  AP890520-0085   82086\n",
       "1  AP891002-0194   39994\n",
       "2  AP880926-0171  161630\n",
       "3  AP890913-0206   72480\n",
       "4  AP880315-0009  113025"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_id_mapping = {}\n",
    "\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "    ext_id_mapping[ext_doc_id] = int_doc_id\n",
    "    \n",
    "ext_id_mapping_df = pd.DataFrame.from_dict(list(ext_id_mapping.items()))\n",
    "ext_id_mapping_df.columns = ['ext_id', 'doc_id']\n",
    "ext_id_mapping_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>rel</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>157141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>117241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>117241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>117270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>94742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id rel  doc_id\n",
       "0      144   0  157141\n",
       "1      144   0  117241\n",
       "2      120   0  117241\n",
       "3      144   0  117270\n",
       "4      144   0   94742"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "with open('ap_88_89/qrel_validation', 'r') as inpt:\n",
    "    reader = list(csv.reader(inpt,delimiter=' '))\n",
    "    # print (reader)\n",
    "    \n",
    "y_val = pd.DataFrame(reader)\n",
    "y_val.columns = ['query_id', '0', 'ext_id', 'rel']\n",
    "y_val = y_val.merge(ext_id_mapping_df)\n",
    "y_val.drop(columns=['0', 'ext_id'], inplace=True)\n",
    "y_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>rel</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>112918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>91625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>91625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>91625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>121400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id rel  doc_id\n",
       "0       68   1  112918\n",
       "1       68   1   91625\n",
       "2      183   0   91625\n",
       "3      121   0   91625\n",
       "4       68   1  121400"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('ap_88_89/qrel_test', 'r') as inpt:\n",
    "    reader = list(csv.reader(inpt,delimiter=' '))\n",
    "    # print (reader)\n",
    "    \n",
    "y_test = pd.DataFrame(reader)\n",
    "y_test.columns = ['query_id', '0', 'ext_id', 'rel']\n",
    "y_test = y_test.merge(ext_id_mapping_df)\n",
    "y_test.drop(columns=['0', 'ext_id'], inplace=True)\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26458\n",
       "1    11530\n",
       "Name: rel, dtype: int64"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_updated = pd.merge(X, y_test, left_on=['doc_id', 'query_id'], right_on=['doc_id', 'query_id'], how='left')\n",
    "X_updated['rel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_updated['rel'].value_counts()[1] / (X_updated['rel'].value_counts()[1] + X_updated['rel'].value_counts()[0])\n",
    "X_updated.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_updated.reset_index(inplace=True, drop=True)\n",
    "X_updated['rel'].unique()\n",
    "X_updated.drop(['query_id', 'doc_id', 'abs_discounting05', 'abs_discounting09', 'dirichlet1000', 'dirichlet1500'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dirichlet500</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>abs_discounting01</th>\n",
       "      <th>bm25</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.245471</td>\n",
       "      <td>4.399897</td>\n",
       "      <td>10.111307</td>\n",
       "      <td>9.039644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.298711</td>\n",
       "      <td>1.255652</td>\n",
       "      <td>4.336253</td>\n",
       "      <td>4.198218</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.026551</td>\n",
       "      <td>4.002170</td>\n",
       "      <td>3.861726</td>\n",
       "      <td>3.938911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.022036</td>\n",
       "      <td>7.425736</td>\n",
       "      <td>3.578382</td>\n",
       "      <td>3.910175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.623746</td>\n",
       "      <td>5.342427</td>\n",
       "      <td>2.661292</td>\n",
       "      <td>3.511884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dirichlet500     tfidf  abs_discounting01      bm25 rel\n",
       "0      9.245471  4.399897          10.111307  9.039644   0\n",
       "1      4.298711  1.255652           4.336253  4.198218   0\n",
       "2      4.026551  4.002170           3.861726  3.938911   1\n",
       "3      4.022036  7.425736           3.578382  3.910175   1\n",
       "4      3.623746  5.342427           2.661292  3.511884   1"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_updated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X_updated.loc[:, X_updated.columns != 'rel']\n",
    "y = X_updated['rel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.69649908  0.69702553  0.69623585  0.69755199  0.69781521  0.69702553\n",
      "  0.69676231  0.69649908  0.69720906  0.69694576]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics, cross_validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# predicted = cross_validation.cross_val_predict(LogisticRegression(), X, y, cv=10)\n",
    "scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69695694011103781"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.700096516627\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      1.00      0.82      7978\n",
      "          1       0.53      0.00      0.00      3419\n",
      "\n",
      "avg / total       0.65      0.70      0.58     11397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11382\n",
       "1       15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predicted).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task 4: Write a report [20 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
